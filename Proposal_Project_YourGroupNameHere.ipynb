{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "https://github.com/kstad21/COGS188_jhek/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Katy Stadler\n",
    "- Elvin Li\n",
    "- Jiasheng Zhou\n",
    "- Harry Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "We propose to examine the effectiveness of several reinforcement learning strategies on the Tetris environment, including deep-Q networks, proximal policy optimization and Monte Carlo Tree Search. Within each RL strategy, we can explore different cost functions and other hyperparameters to gain information about optimal performance parameters for each learning method. To evaluate the overall performance of the algorithms, we will examine both the speed/rate of convergence and overall performance post-convergence, as well as compare the performance of our model to the performance of the benchmark agent given in the environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Tetris is a video game created by a Russian software engineer that became popular worldwide. In the game, there is a grid-environment where differently shaped blocks “fall” from the top of the grid to the bottom. It is the user’s job to arrange the blocks (by moving them left and right and/or rotating them) so that they can survive as long as possible without any of the blocks accumulating all the way to the top of the grid. Rows can be dissolved (therefore giving the user more space) if every single grid in a row is filled by any part of a block (a ‘tetris’ is a quadruple of rows filled and cleared at the same time) [<sup>[1]</sup>](#historyoftetrisnote). Strategy for the game involves trying to maximize the number of cleared rows and tetrises, especially when the goal is to receive as many points as possible. In the situation where the goal is to “stay alive” as long as possible, the main goal is to keep the highest block as low as possible, often by virtue of row-clearings and/or tetrises.\n",
    "\n",
    "A 2013 paper introduced the Arcade Learning Environment (ALE) [<sup>[2]</sup>](#alenote) that we plan to utilize in this project. ALE’s goal is to provide an interface to hundreds of Atari game environments, as well as methods to evaluate and compare approaches used to train agents in these games. There are different methods for feature construction and three simple baseline agents: Random, which picks a random action every frame, Const, which picks a single action throughout an episode, and Perturb, which selects a fixed action with a 95% probability and is uniformly random otherwise. \n",
    "\n",
    "It has been proven that even in an offline version of Tetris, it is NP-complete to “maximize the number of cleared rows, maximize the number of tetrises, minimize the maximum height of an occupied square, or maximize the number of pieces played before the game ends” [<sup>[3]</sup>](#nphardnote). These results held when players were restricted to only 2 rotation/translation moves before each piece drops in height, restricted piece sets, and with an infinitely tall gameboard. This is why we are interested in testing the performance of different models and hyperparameters as we train an agent to play the game.\n",
    "\n",
    "Even before the 2013 ALE was released, there were several attempts at training an agent to play Tetris. In 1996, Tsitsiklis & Van Roy used feature-based dynamic programming (number of holes and height of the highest column) to achieve a score of around 30 cleared lines on a 16x10 grid [<sup>[4]</sup>](#dpnote). In the same year, Bertsekas & Tsitsiklis added the height of each column and the difference in height between adjacent columns as features. They achieved a higher score of 2800 lines using lambda-policy iteration [<sup>[5]</sup>](#lambdaitnote). Later, even further features were added, including mean column height and the sum of the differences in adjacent column height. Least-squares policy iteration achieved an average score of between 1000 and 3000 lines [<sup>[6]</sup>](#leastsquaresitnote). \n",
    "\n",
    "Due to the design of Tetris, it doesn’t really make sense to have a reward function that gives rewards only at the end of the game. One TD(0)-Learning implementation uses linear combinations of weighted features, such as the value of the highest-used column, the average of the heights of all used columns, the number of holes between pieces at each given time, and the “quadratic unevenness” of the profile (which is a result of summing the squared values of the differences of neighboring columns) [<sup>[7]</sup>](#egreedynote). In order to reduce the state space, a constrained height difference between adjacent columns was used to encode each state. In this experiment, it was shown that lower values of ε were beneficial when using an epsilon-greedy policy.\n",
    "\n",
    "Tetris can also be modeled as a Markov Decision process if its state space is somehow reduced. Without reduction, a simple 20x10 board has 2200 ways to fill it and even with the requirement that no row be completely full, this is still (210 - 1)20. This 2022 paper’s [<sup>[8]</sup>](#mdpnote) most successful approach, “Fitted Value Iteration”, chose a small set of features and represented each state in terms of this set of features. This method was contingent upon “featurization” of the MDP and a small number of samples from the original state space needed to represent the state-value relationship. The article also found that the features Max-Height, Num-Holes, and Num-Covers were promising features when it came to training an agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The core problem in this project is developing multiple Reinforcement-Learning based algorithms for learning a successful strategy to play Tetris, the classic block-stacking video game. Since the game is made to run infinitely, approximating reward as well as the action space is the most challenging part of the problem. The state space is so large that some of the algorithms we've learned are not feasible, and state representations can be complicated due to how many possibilities a board can hold. We will use algorithms and approximations that we have learned about in this class or researched (see *Background*) to train an agent to play a simple Tetris (the environment provided in the ALE). Along with this goal, the project aims to compare and contrast the various models, to determine the relative strengths and weaknesses of each strategy in areas such as convergence rate and best score, and compare to the benchmark agents of the ALE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "Since we hope to measure the learning algorithms’ development of strategies, using past data for our experiments doesn’t align with the current goal of measuring RL-strategy effectiveness. Instead, we will generate data dynamically through live simulations, allowing our models to learn through interaction with the environment. One such environment that can foster this is Farama’s tetris environment (https://gymnasium.farama.org/environments/atari/tetris/), which provides a retro Tetris setup for reinforcement learning research. Training models in this environment is analogous to collecting data, as the agent improves its actions and policies through multiple iterations of gameplay, guided by a reward system (e.g. based on the number of rows cleared per episode). The environment offers a discrete action space with five possible moves: move left, move right, drop down, rotate, and no operation (NOOP). The game state is represented through an interface that can be launched in Python, with observations encoded as RGB pixel values.\n",
    "\n",
    "If we have time to expand on the current topic, we plan to introduce a neural network model that could theoretically learn from professional-level tetris gameplay, but this is currently out of scope for our proposed question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "In this section, clearly describe a solution to the problem. The solution should be applicable to the project domain and appropriate for the dataset(s) or input(s) given. Provide enough detail (e.g., algorithmic description and/or theoretical properties) to convince us that your solution is applicable. Why might your solution work? Make sure to describe how the solution will be tested.  \n",
    "\n",
    "If you know details already, describe how (e.g., library used, function calls) you plan to implement the solution in a way that is reproducible.\n",
    "\n",
    "If it is appropriate to the problem statement, describe a benchmark model<a name=\"sota\"></a>[<sup>[3]</sup>](#sotanote) against which your solution will be compared. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our solution will develop an effective Tetris-playing agent using deep reinforcement learning methods, specifically Deep Q-Networks, Proximal Policy Optimization, and Monte Carlo Tree Search. Initially, we considered Markov Decision Processes, Q-learning, and SARSA, but these approaches are infeasible due to the massive state space and sparse rewards in Tetris. Tabular Q-learning is impractical because the number of board configurations grows exponentially. SARSA struggles with long-term planning since rewards are sparse and only given when a row clears. Standard MDP-based approaches are computationally expensive due to the large number of possible states and transitions.\n",
    "\n",
    "To address these challenges, we will implement deep Q-networks, which approximate Q-values using a convolutional neural network instead of a lookup table, making them well-suited for pixel-based environments like our Tetris game. We will also use proximal policy optimization, a policy-based reinforcement learning method that directly learns a policy instead of estimating Q-values. Additionally, we will incorporate Monte Carlo Tree Search to enable lookahead-based decision-making. Since Tetris requires strategic planning to avoid early termination and maximize line clears, MCTS can be leveraged to simulate short-term sequences of piece placements and evaluate their potential impact.\n",
    "\n",
    "Each model’s effectiveness will be evaluated using key metrics such as maximum achievable score, average game duration, and total rows cleared per episode. The implementation will be fully reproducible using Stable-Baselines3, PyTorch, and Tensorflow as our main frameworks. As the project progresses, we plan to introduce additional models, such as more advanced policy-gradient methods or model-based reinforcement learning approaches to further explore alternative strategies. These models will be incorporated incrementally, allowing us to compare their effectiveness against our initial baselines and refine our approach based on observed results. Lastly, we will evaluate our model with the baseline models provided in the ALE environment (see *Background* for more details). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "One key evaluation metric for quantifying the performance of both the benchmark model and the solution model is the total number of rows cleared per episode. This metric directly reflects the agent’s ability to play Tetris effectively, as a higher number of cleared rows indicates better performance by demonstrating that the agent has learned to optimize piece placement and avoid early game termination. \n",
    "\n",
    "We will also assess the average game duration, measured in the number of steps per episode, to determine how long the agent is able to sustain gameplay. A well-performing agent should be able to survive longer while also maximizing row clears. \n",
    "\n",
    "Another useful metric is the maximum achievable score, which incorporates both survival time and efficiency in clearing rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your project has obvious potential concerns with ethics or data privacy discuss that here.  Almost every ML project put into production can have ethical implications if you use your imagination. Use your imagination. Get creative!\n",
    "\n",
    "Even if you can't come up with an obvious ethical concern that should be addressed, you should know that a large number of ML projects that go into producation have unintended consequences and ethical problems once in production. How will your team address these issues?\n",
    "\n",
    "Consider a tool to help you address the potential issues such as https://deon.drivendata.org\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there seem to be few privacy implications of training an agent to play Tetris using reinforcement learning, the larger concept of training bots to play games does have ethical implications. Simple games like Tetris, Minesweeper, even slither.io are special/nostalgic for many people because of the way we grew up playing them. These games were designed by humans for humans, and one could argue that training bots to beat these games goes against the spirit of the games themselves. Now, with AI becoming more popular and more and more environments being provided to easily train agents to play games, the spirit of the game has definitely changed. As an extreme, this thought process contrasts the way we (and generations before) were animatedly playing slither.io in the middle of class with our friends or playing games at the arcade, versus newer generations, who may be more inclined to just train a bunch of bots without even playing for themselves. \n",
    "\n",
    "Also importantly, to avoid the possibility of a rogue artificial superintelligence, we will avoid giving our models internet-access capabilities during the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Communications through group chat via Instagram and email. Expected to be addressed within 24 hours.*\n",
    "* *We will create a schedule on sheets to keep track of course deadlines and self-set deadlines, with assignments for a specific member if applicable.*\n",
    "* *It is understand that members may have different strengths/expertise, and it is expected that everyone contributes substantially to the project so that the work is evenly divided.*\n",
    "* *Team or scheduling issues will be addressed in a group meeting, either in person or via Zoom. We prefer for in-depth issues to be discussed face-to-face.*\n",
    "* *Week-to-week goals will be set and there will be weekly checkins to update the spreadsheet and check in on progress.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 3/04  |  12 PM | Complete proposal with new topic  | Resubmit proposal to Scott | \n",
    "| 3/08  |  6 PM |  Get a simple Q-learning model to train, each person should also get the environment working and investigate | Things that worked and things that didn't work; next steps | \n",
    "| 3/12  | 6:30 PM  | Implement more of the models  | Discuss any problems and possible ways to refine the models |\n",
    "| 3/16  | 6:30 PM  | Refine models and evaluation metrics | Make sure work is combined seamlessly  |\n",
    "| 3/17  | 6:30 PM  | Next steps to start report | Delegate work for report |\n",
    "| 3/19  | 6:30 PM  | Have the writeup done before the due date | Use the extra time to check over the writeup  |\n",
    "| 3/19  | 11:15 PM  | Turn it in!  | N/A |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<!--\n",
    "<a name=\"lorenznote\"></a>1.[^](#lorenz): Lorenz, T. (9 Dec 2021) Birds Aren’t Real, or Are They? Inside a Gen Z Conspiracy Theory. *The New York Times*. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html<br> \n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n",
    "-->\n",
    "\n",
    "<a id=\"historyoftetrisnote\"></a>  \n",
    "**¹** Weisberger, M. (13 Oct 2016) The Bizarre History of 'Tetris'. *LiveScience*. https://www.livescience.com/56481-strange-history-of-tetris.html <br>\n",
    "<a id=\"alenote\"></a> **²** Bellemare, M. et al. (14 Jun 2013) The Arcade Learning Environment: An Evaluation Platform for General Agents. *Journal of Artificial Intelligence Research*. https://jair.org/index.php/jair/article/view/10819 <br>\n",
    "<a id=\"nphardnote\"></a> **³** Demaine, E. et al. (21 Oct 2002) Tetris is Hard, Even to Approximate. arXiv.org. https://arxiv.org/abs/cs/0210020 <br>\n",
    "<a id=\"dpnote\"></a> **⁴** Tsitsiklis, J., Van Roy, B. (5 May 1996) An Analysis of Temporal-Difference Learning with Function Approximation. *IEEE TRANSACTIONS ON AUTOMATIC CONTROL*. https://www.mit.edu/~jnt/Papers/J063-97-bvr-td.pdf <br>\n",
    "<a id=\"lambdaitnote\"></a> **⁵** Bertsekas, D., Tsitsiklis, J. (1996) Neuro-Dynamic Programming. *Athena Scientific* <br>\n",
    "<a id=\"leastsquaresitnote\"></a> **⁶** Lagoudakis, M. et al. (2002) Least-squares methods in reinforcement learning for control. *Hellenic Conference on Artificial Intelligence* <br>\n",
    "<a id=\"egreedynote\"></a> **⁷** Thiam, P. et al. (2014) A Reinforcement Learning Algorithm to Train a Tetris Playing Agent. *Artificial Neural Networks in Pattern Recognition*. https://link.springer.com/chapter/10.1007/978-3-319-11656-3_15 <br>\n",
    "<a id=\"mdpnote\"></a> **⁸** Bodoia, M., Puranik, A. (2022) Applying Reinforcement Learning to Competitive Tetris. https://cs229.stanford.edu/proj2012/BodoiaPuranik-ApplyingReinforcementLearningToCompetitiveTetris.pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
